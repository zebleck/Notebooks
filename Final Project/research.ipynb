{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible papers:\n",
    "\n",
    "[Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)<br>\n",
    "[Deep Transformer Models for Time Series Forecasting:\n",
    "The Influenza Prevalence Case](https://arxiv.org/pdf/2001.08317.pdf)<br>\n",
    "[Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907)<br>\n",
    "[Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks](https://royalsocietypublishing.org/doi/10.1098/rspa.2017.0844)<br>\n",
    "[Long Expressive Memory for Sequence Modeling](https://github.com/tk-rusch/LEM)<br>\n",
    "[Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.024102)<br>\n",
    "[Next Generation Reservoir Computing](https://www.nature.com/articles/s41467-021-25801-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case\n",
    "\n",
    "## State Space Model (SSM) models:\n",
    "\n",
    "### Compartmental Models\n",
    "\n",
    "for example the SIR model. While compartmental models are useful, they require prior knowledge on the parameters of the differential equations and they lack flexbility of updating parameters upon new observations.\n",
    "\n",
    "### ARIMA\n",
    "\n",
    "ARIMA models the observed variable $x_t$ and assumes $x_t$ can be decomposed into trend, seasonal and irregular components. First, the time series is differenced in order to eliminate trend and seasonality. Then, the irregular component is modeled using an ARMA model. ARIMA is a kind of blackbox approach, where the model purely depends on the observed data and has no analysis of the states of the underlying systems.\n",
    "\n",
    "### Time Delay Embedding\n",
    "\n",
    "For a scalar time series $x_t$, the TDE is formed by embedding each scalar value $x_t$ into a d-dimensional time-delay space:\n",
    "$$\\text{TDE}_{d,\\tau}(x_t)=(x_t, x_{t-\\tau},\\dots,x_{t-(d-1)\\tau})$$\n",
    "For any non-linear dynamical systems, Takens' theorem that there exists a certain $(d,\\tau)$-time delay embedding such that the evolution of the evolution of the original state variables (\"phase space\") can be recovered in the delay coordinates of the observed variables. This means that the TDE can be used to approximate the underlying dynamics of the system.\n",
    "\n",
    "## Sequence models\n",
    "\n",
    "Many real-world ML tasks deal with sequential data such as natural language text, audio, video and time series data.\n",
    "\n",
    "### RNNs\n",
    "\n",
    "$$h_t=\\sigma (W_x x_t + W_h h_{t-1} + b_h)$$\n",
    "$$y_t = \\sigma (W_y h_t + b_y)$$\n",
    "\n",
    "Where $W_x$ is the input weight matrix, $W_h$ is the hidden weight matrix, $W_y$ is the output weight matrix, $b_h$ is the hidden bias vector, $b_y$ is the output bias vector, $x_t$ is the input vector at time step $t$, $h_t$ is the hidden state vector at time step $t$, $y_t$ is the output vector at time step $t$ and $\\sigma$ is the activation function.y\n",
    "\n",
    "Suffers from vanishing gradient problem.\n",
    "\n",
    "### LSTMs\n",
    "\n",
    "Specifically developed to address the vanishing gradient problem in RNNs. It employs three gates, including an input gate, forget gate and output gate.\n",
    "\n",
    "### Seq2Seq\n",
    "\n",
    "Seq2Seq is a sequence-to-sequence model that consists of two RNNs: an encoder and a decoder. The encoder reads the input sequence and produces a fixed-length vector representation of the input sequence. The decoder reads the vector representation and produces the output sequence.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "The Transformer is a sequence-to-sequence model that consists of an encoder and a decoder. The encoder is composed of an input layer, a positional encoding layer and a stack of four identical encoder layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
