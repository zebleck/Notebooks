
@article{rusch_long_2022,
	title = {{LONG} {EXPRESSIVE} {MEMORY} {FOR} {SEQUENCE} {MODELING}},
	abstract = {We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efﬁciently process sequential tasks with very long-term dependencies, and it is sufﬁciently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a wellknown challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classiﬁcation through dynamical systems prediction to keyword spotting and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.},
	language = {en},
	author = {Rusch, T Konstantin and Mishra, Siddhartha and Mahoney, Michael W},
	year = {2022},
	file = {Rusch et al. - 2022 - LONG EXPRESSIVE MEMORY FOR SEQUENCE MODELING.pdf:C\:\\Users\\Kontor\\Zotero\\storage\\FC5NMAQ4\\Rusch et al. - 2022 - LONG EXPRESSIVE MEMORY FOR SEQUENCE MODELING.pdf:application/pdf},
}
